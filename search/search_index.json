{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pando85's Homelab","text":"<p>This project utilizes Infrastructure as Code and GitOps to automate provisioning, operating, and updating self-hosted services in my homelab. Based in K3s, ArgoCD, Renovate and ZFS. It can be used as a highly customizable framework to build your own homelab.</p> <p>What is a homelab?</p> <p>Homelab is a laboratory at home where you can self-host, experiment with new technologies, practice for certifications, and so on. For more information about homelab in general, see the r/homelab introduction.</p>"},{"location":"#overview","title":"\ud83d\udcd6 Overview","text":"<p>This section provides a high level overview of the project. For further information, please see the documentation.</p>"},{"location":"#kubernetes","title":"\u26f5 Kubernetes","text":"<p>This repo is focused in maintain in a GitOps practical way my home infrastructure. Ansible is used to deploy a simple K3s cluster. Managed by ArgoCD.</p>"},{"location":"#installation","title":"Installation","text":"<p>The cluster is running on Debian based distributions, deployed on bare-metal. We use custom Ansible playbooks and roles to setup the Kubernetes cluster.</p>"},{"location":"#core-components","title":"Core components","text":"<ul> <li>external-secrets: External Secrets   Operator reads information from a Vault and automatically injects the values as Kubernetes   Secrets.</li> <li>hashicorp/vault: A tool for secrets management, encryption as a   service, and privileged access management.</li> <li>kubernetes-sigs/external-dns: Automatically   manages DNS records from my cluster in a cloud DNS provider.</li> <li>jetstack/cert-manager: Creates SSL certificates for services in   my Kubernetes cluster.</li> <li>kubernetes/ingress-nginx: Ingress controller to   expose HTTP traffic to pods over DNS.</li> <li>openebs/zfs-localpv: CSI Driver for dynamic   provisioning of Persistent Local Volumes for Kubernetes using ZFS.</li> <li>kanidm: A simple, secure and fast identity management platform.</li> <li>velero: Tool to safely backup and restore, perform disaster recovery,   and migrate Kubernetes cluster resources and persistent volumes.</li> </ul>"},{"location":"#hardware","title":"\ud83d\udd27 Hardware","text":"Hostname Device Count OS Disk Size Data Disk Size Ram Operating System Purpose grigri Supermicro Atom C2758 (A1SRi-2758F) 1 250GB SSD 3*4TB + 500GB (NVMe) RAIDZ + cache 32GB Ubuntu 22.04 K3s server prusik* Ryzen 9 7950X (ASUS PRIME X670-P) 1 512GB 4*12TB + 2TB (NVMe) RAIDZ + cache 64GB Ubuntu 24.04 k3s agent k8s-odroid-hc4-3 Odroid-hc4 1 N/A N/A 4GB Armbian K3s agent prusik-ipmi Raspberry Pi 4 Model B Rev 1.5 1 16GB N/A 4GB PiKVM ipmi pfsense PC Engines APU2e4 1 60GB N/A 4GB pfSense/FreeBSD Router gs724t Netgear gs724t 1 N/A N/A N/A N/A Switch cerezo Unifi UAP 1 N/A N/A N/A N/A AP manzano Unifi UAP 1 N/A N/A N/A N/A AP <p>* with Nvidia GeForce GTX 1060 3GB</p>"},{"location":"#images","title":"Images","text":""},{"location":"#features","title":"\u2b50 Features","text":"<ul> <li> Common applications: Jellyfin, Gitea, arr, Nextcloud...</li> <li> Automated Kubernetes installation and management</li> <li> Installing and managing applications using GitOps</li> <li> Automatic rolling upgrade for OS and Kubernetes</li> <li> Automatically update apps (with approval if needed)</li> <li> Modular architecture, easy to add or remove features/components</li> <li> Automated certificate management</li> <li> Automatically update DNS records for exposed services</li> <li> Monitoring and alerting</li> <li> Single sign-on</li> <li> Automated backups</li> </ul>"},{"location":"#dns","title":"\ud83c\udf10 DNS","text":"<p>ExternalDNS is deployed in the cluster and configured to sync DNS records to Cloudflare.</p> <p>All connections outside the cluster are handled with TLS using cert-manager with Let's Encrypt.</p>"},{"location":"#load-balancer","title":"Load Balancer","text":"<p>Cilium is configured with BGP control plane, both on my router and within the Kubernetes cluster.</p>"},{"location":"#ingress-controllers","title":"Ingress Controllers","text":"<p>For external access, port forwarding is configured for ports <code>80</code> and <code>443</code>, directing traffic to the load balancer IP of the Kubernetes ingress controller.</p> <p>There are also another ingress controller for internal use.</p>"},{"location":"#internal-dns","title":"Internal DNS","text":"<p><code>internal.grigri.cloud</code> domain is used. Configured as:</p> <pre><code>annotations:\n  cert-manager.io/cluster-issuer: letsencrypt-prod-dns\n  external-dns.alpha.kubernetes.io/enabled: \"true\"\n</code></pre>"},{"location":"#external-dns","title":"External DNS","text":"<p><code>grigri.cloud</code> domain is used. Configured as:</p> <pre><code>annotations:\n  cert-manager.io/cluster-issuer: letsencrypt-prod-dns\n  external-dns.alpha.kubernetes.io/enabled: \"true\"\n  external-dns.alpha.kubernetes.io/target: grigri.cloud\n</code></pre>"},{"location":"#thanks","title":"\ud83e\udd1d Thanks","text":"<p>Thanks to all folks who donate their time to the Kubernetes @Home community. A lot of inspiration for my cluster came from those that have shared their clusters over at awesome-home-kubernetes.</p>"},{"location":"lidarr/","title":"Lidarr","text":""},{"location":"lidarr/#change-metadata-source-in-sqlite-db-via-ephemeral-debug-container","title":"Change Metadata Source in SQLite DB via Ephemeral Debug Container","text":"<p>This guide documents how to update or revert the <code>metadatasource</code> value in the Lidarr SQLite database using a debug container in Kubernetes (context: <code>grigri</code>).</p> <p>Source: https://github.com/blampe/hearring-aid</p>"},{"location":"lidarr/#steps-to-change-metadata-source","title":"Steps to Change Metadata Source","text":"<ol> <li>Start a debug container with SQLite3:</li> </ol> <pre><code>kubectl --context=grigri debug -n lidarr -it lidarr-0 --image=nouchka/sqlite3 --target=lidarr -- /bin/sh\n</code></pre> <p>If you don't see a command prompt, try pressing enter.</p> <ol> <li>(Optional) Start bash for convenience:</li> </ol> <pre><code>bash\n</code></pre> <ol> <li>Copy the database to <code>/tmp</code>:</li> </ol> <pre><code>cp /proc/1/root/config/lidarr.db /tmp/\n</code></pre> <ol> <li>Open the database:</li> </ol> <pre><code>sqlite3 /tmp/lidarr.db\n</code></pre> <ol> <li>Run the SQL to change the metadata source:</li> </ol> <pre><code>INSERT INTO Config (Key, Value) VALUES ('metadatasource', 'https://api.musicinfo.pro/api/v0.4/');\n</code></pre> <ol> <li>Exit SQLite3 and copy the DB back:    <pre><code>cp /tmp/lidarr.db /proc/1/root/config/lidarr.db\n</code></pre></li> </ol>"},{"location":"lidarr/#steps-to-revert-the-change","title":"Steps to Revert the Change","text":"<p>Repeat steps 1\u20134 above, then:</p> <ol> <li>Run the SQL to revert:</li> </ol> <pre><code>DELETE FROM Config WHERE Key = 'metadatasource';\n</code></pre> <ol> <li>Exit SQLite3 and copy the DB back:    <pre><code>cp /tmp/lidarr.db /proc/1/root/config/lidarr.db\n</code></pre></li> </ol>"},{"location":"lidarr/#configure-tubifarry-plugin-in-lidarr","title":"Configure Tubifarry Plugin in Lidarr","text":"<ol> <li>Navigate to System &gt; Plugins.</li> <li>Install Tubifarry prod plugin by entering the URL:</li> </ol> <pre><code>https://github.com/TypNull/Tubifarry\n</code></pre> <p>and click Install.</p> <ol> <li>After Lidarr restarts, go back to System &gt; Plugins.</li> <li>Install the Tubifarry develop branch plugin by entering:</li> </ol> <pre><code>https://github.com/TypNull/Tubifarry/tree/develop\n</code></pre> <p>and click Install.</p> <ol> <li>After Lidarr restarts, log back into Lidarr and go to Settings &gt; Metadata.</li> <li>Under Metadata Consumers, select Lidarr Custom.</li> <li>Check both boxes and enter your metadata server URL (e.g., <code>http://host_ip:5001</code>) in the    Metadata Source field.</li> <li>Save changes and restart Lidarr again.</li> </ol>"},{"location":"deployment/cilium-bgp-control-plane/","title":"Cilium BPG control plane","text":"<p>Cilium replaces MetalLB for creating K8s LBs based on BGP.</p>"},{"location":"deployment/cilium-bgp-control-plane/#deployment","title":"Deployment","text":"<p>Ansible set up the configuration and ArgoCD deploys Cilium.</p>"},{"location":"deployment/cilium-bgp-control-plane/#router-config","title":"Router config","text":"<p>In order to use Cilium BGP control plane mode we must configure Pfsense as router to be able of share BGP route table and route all network to that load balancer IPs. We use this tutorial</p> <ul> <li> <p>install package ffr</p> </li> <li> <p>configure <code>Services-&gt;FRR-&gt;Global Settings</code>:</p> </li> </ul> <pre><code>[general options]\nenable=x\ndefault_router_id=192.168.192.1\n</code></pre> <ul> <li><code>Services-&gt;FRR-&gt;Global Settings-&gt;Route Maps</code>:</li> </ul> <pre><code>- name: allow-all\n  description: Match any route\n  action: permit\n  Sequence: 100\n</code></pre> <ul> <li><code>Services-&gt;FRR-&gt;BGP-&gt;BGP</code>:</li> </ul> <pre><code>[bgp router options]\nenable=x\nlocal_as=64512\n\n[graceful restart/shutdown]\nenable_bgp_graceful_shutdown=true\n</code></pre> <ul> <li>in <code>Services-&gt;FRR-&gt;BGP-&gt;Neighbors</code>:</li> </ul> <pre><code>- name: 192.168.192.2\n  descr: grigri\n  remote_as: 64513\n  next_hop_self: true\n  route_map_filters:\n    inbound_router_map_filter: allow-all\n    outbound_router_map_filter: allow-all\n  allow_as_inbound: enabled\n- name: 192.168.192.3\n  descr: prusik\n  remote_as: 64513\n  next_hop_self: true\n  route_map_filters:\n    inbound_router_map_filter: allow-all\n    outbound_router_map_filter: allow-all\n  allow_as_inbound: enabled\n- name: 192.168.192.23\n  descr: k8s-odroid-hc4-3\n  remote_as: 64513\n  next_hop_self: true\n  route_map_filters:\n    inbound_router_map_filter: allow-all\n    outbound_router_map_filter: allow-all\n  allow_as_inbound: enabled\n</code></pre> <p>Important: to access Cilium IP pools network from kubernetes subnet you need to add your host to bgp Some issues could be experimented if not added as <code>docker push</code> not working correctly.</p>"},{"location":"deployment/manual-setup/","title":"Manual setup","text":"<ul> <li>Network</li> <li>Pfsense</li> <li>Servers</li> <li>Flash SDs<ul> <li>amd64 instances</li> <li>odroid-hc4</li> <li>Bootloader Bypass Method</li> <li>Naming convention</li> </ul> </li> <li>Troubleshooting<ul> <li>Same mac problem</li> <li>No python interpreter found</li> </ul> </li> <li>Setup</li> <li>Cluster</li> </ul>"},{"location":"deployment/manual-setup/#network","title":"Network","text":""},{"location":"deployment/manual-setup/#pfsense","title":"Pfsense","text":"<ul> <li>Connect to DMZ <code>192.168.192.0/24</code></li> <li>Add DHCP server: range(60-99), but fix agent IPs before add to the cluster.</li> <li>only for controller HA - Create lb for apiserver (used HaProxy: increase client, server and   tunnel* timeouts to 86400000)</li> <li>Add DNS entry</li> </ul> <p>tunnel*: must be added in <code>backend-&gt;advanced settings-&gt;backend pass thru</code> as <code>timeout tunnel 86400s</code></p>"},{"location":"deployment/manual-setup/#servers","title":"Servers","text":""},{"location":"deployment/manual-setup/#flash-sds","title":"Flash SDs","text":"<ul> <li>odroid-hc4: image</li> <li>amd64: usb-stick</li> <li>grigri: usb-stick</li> <li>prusik-ipmi: image</li> </ul> <p>Use script from <code>scripts/prepare_sdcard.sh</code> to prepare instances. amd64 and grigri should be installed manually.</p>"},{"location":"deployment/manual-setup/#amd64-instances","title":"amd64 instances","text":"<p>Installed with Ubuntu: select ubuntu server (non minimized) and follow the process.</p>"},{"location":"deployment/manual-setup/#odroid-hc4","title":"odroid-hc4","text":"<p>Important: To be able to boot clean Armbian mainline based u-boot / kernel experiences, you need to remove incompatible Petitboot loader that is shipped with the board.</p>"},{"location":"deployment/manual-setup/#bootloader-bypass-method","title":"Bootloader Bypass Method","text":"<p>This is now the preferred method. It is easier, and be performed without a display via SSH</p> <p>Install an SD Card with a fresh Armbian image Flip device upside down With a tool, press and hold down the black button. Continue holding button and plug in power to device Login to console or SSH and perform follow normal setup procedures Verify system can access SPI FLASH device and Erase Reboot</p> <pre><code>odroidhc4:~:# ls -ltr /dev/mtd*\ncrw------- 1 root root 90, 0 Nov  6 21:38 /dev/mtd0\nbrw-rw---- 1 root disk 31, 0 Nov  6 21:38 /dev/mtdblock0\ncrw------- 1 root root 90, 1 Nov  6 21:38 /dev/mtd0ro\nodroidhc4:~:# flash_erase /dev/mtd0 0 0\nErasing 4 Kibyte @ fff000 -- 100 % complete\nodroidhc4:~:#\n</code></pre>"},{"location":"deployment/manual-setup/#naming-convention","title":"Naming convention","text":"<p>All nodes must be named with prefix <code>k8s-{hardware_tag}-{numerical_id}</code>. For example:</p> <ul> <li>k8s-odroid-hc4-1</li> <li>k8s-amd64-1</li> </ul> <p>Also, consider their use case and performance profile. For example, for Ceph nodes:</p> <ul> <li>k8s-sas-ssd-1</li> <li>k8s-hot-storage-2</li> </ul>"},{"location":"deployment/manual-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/manual-setup/#same-mac-problem","title":"Same mac problem","text":"<p>Editing <code>/boot/ArmbianEnv.txt</code> didn't work.</p> <p><code>/etc/network/interfaces</code>:</p> <pre><code>...\nauto eth0\niface eth0 inet dhcp\n  hwaddress ether b6:09:a4:06:00:8b\n</code></pre>"},{"location":"deployment/manual-setup/#no-python-interpreter-found","title":"No python interpreter found","text":"<pre><code>ln -s /usr/bin/python3 /usr/bin/python\n</code></pre>"},{"location":"deployment/manual-setup/#setup","title":"Setup","text":"<p>Use playbook <code>playbooks/install/so.yml</code> to setup servers.</p> <pre><code>make first-boot\n</code></pre> <p>Note: Armbian default user/password -&gt; root/1234</p>"},{"location":"deployment/manual-setup/#cluster","title":"Cluster","text":"<p><code>playbooks/install/cluster.yml</code> to setup Kubernetes.</p> <pre><code>ansible-playbook playbooks/install/k8s.yml --become\n</code></pre>"},{"location":"deployment/unifi/","title":"UniFi","text":""},{"location":"deployment/unifi/#config-dns","title":"Config DNS","text":"<ul> <li><code>ServicesDNS-&gt;ResolverGeneral-&gt;Settings-&gt;Host Overrides</code>:</li> </ul> <pre><code>- host: unifi-controller\n  domain: grigri\n  ip_address: 192.168.193.2\n  description: Deployed on k8s.grigri in his own load balancer\n  additional_names_for_this_host:\n    - host: unifi\n      domain: grigri\n      description: Used for automatically adopt devices\n</code></pre> <pre><code>\u00bb dog unifi-controller.grigri\nA unifi-controller.grigri. 1h00m00s   192.168.193.2\n\u00bb dog unifi\nA unifi.grigri. 59m50s   192.168.193.2\n</code></pre>"},{"location":"deployment/unifi/#adopt-devices","title":"Adopt devices","text":"<ul> <li>Factory reset. One of this:</li> <li>10 seconds button</li> <li>ssh and run <code>syswrapper.sh restore-default</code></li> <li>Connect to the DMZ network.</li> <li>Click adopt from unifi web interface</li> </ul> <p>Note: For wireless mesh use same channel for both APs (disable Channel Optimization). If still doesn't work go to legacy UI and set up same Channel and width in <code>Settings-&gt;RADIOS</code></p>"},{"location":"deployment/unifi/#ssh-access","title":"ssh access","text":"<p>Add SSH key to <code>System-&gt;Network Device SSH Authentication-&gt;SSH Keys</code> or use the configured password (<code>iot/unifi-device-authentication</code>).</p> <pre><code>ssh {{ hostname }}\n</code></pre>"},{"location":"deployment/unifi/#layer-3-uap-adoption","title":"Layer 3 UAP adoption","text":"<p>From this doc, using SSH to adopt devices from the DMZ network.</p> <p>After SSH into the device:</p> <pre><code>set-inform http://unifi-controller.grigri:8080/inform\n</code></pre> <p>And then you will see the device now show up for adoption.</p>"},{"location":"network/configuration/","title":"Configuration","text":""},{"location":"network/configuration/#diagram","title":"Diagram","text":""},{"location":"network/configuration/#pfsense","title":"pfSense","text":"<p>Manual configuration and backups are managed on the host <code>prusik</code>.</p> <ul> <li>Backup user: Created via   <code>metal/roles/setup/tasks/backup-user.yml</code></li> <li>Backup configuration: See   <code>metal/playbooks/install/backups.yml</code> (partially   manual)</li> </ul>"},{"location":"network/configuration/#disaster-recovery-workflow","title":"Disaster Recovery Workflow","text":"<p>1. Retrieve the disaster recovery pfSense configuration backup:</p> <pre><code>pass grigri/pfsense.config.xml &gt; /tmp/pfsense.config.xml\n</code></pre> <p>2. Reinstalling pfSense:</p> <ul> <li>Connect a null modem RS232 serial cable to the device.</li> <li>On Linux, open the serial console:</li> </ul> <pre><code>sudo picocom -b 115200 /dev/ttyUSB0\n# Alternatively:\n# screen /dev/ttyUSB0 115200\n</code></pre> <ul> <li> <p>Use a pfSense USB installer image. Configure the WAN interface for PPPoE during setup.</p> </li> <li> <p>WAN: <code>igb0</code></p> </li> <li> <p>PPPoE credentials: stored in <code>pass grigri/digi_pppoe</code></p> </li> <li> <p>If the standard installer is unavailable, download an offline ISO from   PfSense old images.</p> </li> <li>Create a bootable USB stick using Etcher.</li> </ul> <p>3. Installation steps:</p> <ul> <li>Insert the USB stick into the router.</li> <li>Connect via serial console and follow the pfSense installation prompts.</li> </ul> <p>4. Restore configuration:</p> <ul> <li>After installation, restore the configuration from the XML backup file.</li> </ul>"},{"location":"network/configuration/#gs724t","title":"gs724t","text":"<ul> <li><code>Ipv4 Network Interface Configuration -&gt; IP Configuration</code>:</li> </ul> <pre><code>- current_network_configuration_protocol: static\n  ip_address: 192.168.76.10\n  subnet_mask: 255.255.255.0\n  default_gateway: 192.168.76.254\n</code></pre> <ul> <li> <p><code>Switching -&gt; VLAN</code>:</p> </li> <li> <p>Add <code>101</code> DMZ</p> </li> <li>Add <code>501</code> IoT</li> <li> <p>Add <code>502</code> IoT Offline</p> </li> <li> <p><code>Switching -&gt; VLAN -&gt; Advanced -&gt; VLAN Membership</code>:</p> </li> </ul> <pre><code>VLAN ID: 1\nVLAN Name: Default\nVLAN Type: Default\nPort  1   2   3   4   5   6   7   8   9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24\n                                                         U    U    U    U    U    U    U    U    U    U    U    U\n</code></pre> <pre><code>VLAN ID: 101\nVLAN Name: DMZ\nVLAN Type: Static\nPort  1   2   3   4   5   6   7   8   9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24\n      U   U   U   U   U   U   U   U   U   U    U    U\n</code></pre> <pre><code>VLAN ID: 501\nVLAN Name: IoT\nVLAN Type: Static\nPort  1   2   3   4   5   6   7   8   9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24\n                                                                                                      T    T    T\n</code></pre> <pre><code>VLAN ID: 502\nVLAN Name: IoT Offline\nVLAN Type: Static\nPort  1   2   3   4   5   6   7   8   9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24\n                                                                                                      T    T    T\n</code></pre> <ul> <li><code>Switching -&gt; VLAN -&gt; Advanced -&gt; Port PVID Configuration</code>:</li> </ul> <pre><code>g1   101   101   101   None   Admit All   Disable   Disable   0\ng2   101   101   101   None   Admit All   Disable   Disable   0\ng3   101   101   101   None   Admit All   Disable   Disable   0\ng4   101   101   101   None   Admit All   Disable   Disable   0\ng5   101   101   101   None   Admit All   Disable   Disable   0\ng6   101   101   101   None   Admit All   Disable   Disable   0\ng7   101   101   101   None   Admit All   Disable   Disable   0\ng8   101   101   101   None   Admit All   Disable   Disable   0\ng9   101   101   101   None   Admit All   Disable   Disable   0\ng10   101   101   101   None   Admit All   Disable   Disable   0\ng11   101   101   101   None   Admit All   Disable   Disable   0\ng12   101   101   101   None   Admit All   Disable   Disable   0\ng13   1   1   1   None   Admit All   Disable   Disable   0\ng14   1   1   1   None   Admit All   Disable   Disable   0\ng15   1   1   1   None   Admit All   Disable   Disable   0\ng16   1   1   1   None   Admit All   Disable   Disable   0\ng17   1   1   1   None   Admit All   Disable   Disable   0\ng18   1   1   1   None   Admit All   Disable   Disable   0\ng19   1   1   1   None   Admit All   Disable   Disable   0\ng20   1   1   1   None   Admit All   Disable   Disable   0\ng21   1   1   1   None   Admit All   Disable   Disable   0\ng22   1   1   1,501-502   501-502   Admit All   Disable   Disable   0\ng23   1   1   1   None   Admit All   Disable   Disable   0\ng24   1   1   1,101,501-502   101,501-502   Admit All   Disable   Disable   0\ng25   1   1   1   None   Admit All   Disable   Disable   0\ng26   1   1   1   None   Admit All   Disable   Disable   0\n</code></pre> <p>Summary:</p> <pre><code>port 1-12: DMZ (VLAN 101)\nport 13-21: LAN (VLAN 1/default)\nport 22-23: LAN (VLAN 1 - Untagged, VLAN 501 - Tagged, VLAN 502 - Tagged)\nport 24: LAN (VLAN 1 - Untagged, VLAN101 - Tagged, VLAN 501 - Tagged, VLAN 502 - Tagged)\n</code></pre>"},{"location":"troubleshooting/major-page-faults/","title":"Major page faults","text":""},{"location":"troubleshooting/major-page-faults/#summary","title":"Summary","text":"<p><code>grigri</code> server is suffering a high rate of major page faults since it was reinstalled.</p>"},{"location":"troubleshooting/major-page-faults/#timeline","title":"Timeline","text":"<p>2023-08-21:</p> <pre><code>- 22:57:XX CEST - Ubuntu 22.04 installed.\n- 23:34:07 CEST - Added to the cluster as worker.\n</code></pre> <p>2023-08-22:</p> <pre><code>- 06:XX:XX CEST - Max ~500 major page faults per second. 95% of memory utilization.\n- 18:40:XX CEST - `grigri` becomes K3s server.\n</code></pre> <p>2023-09-05:</p> <pre><code>- Reduce `zfs_arc_max` to keep under 90% of memory usaged. This fixed major page faults.\n</code></pre> <p>2023-09-06:</p> <pre><code>- 09:14:XX CEST - Increase `zfs_arc_max` and set up `zfs_arc_sys_free` (3.2Gi) to try to avoid having free memory but still ensuring that major page faults are not happening any more.\n- 12:30:XX CEST - Increase `zfs_arc_sys_free` to 4Gi cause memory available is ~ 1.5Gi.\n- 16:22:XX CEST - Remove `zfs_arc_sys_free` and decrease `zfs_arc_max` to 12Gi.\n</code></pre>"},{"location":"troubleshooting/major-page-faults/#root-cause-analysis","title":"Root cause analysis","text":"<p>High memory usage is causing this major page faults. We still have to investigate why 95% seems the threshold for this system.</p>"},{"location":"troubleshooting/mqtt/","title":"MQTT","text":"<p>Connect to server mosquitto with web client:</p> <pre><code>docker run --rm --name mqttx-web -p 80:80 emqx/mqttx-web\n</code></pre> <pre><code>name: random\nclient_id: random\nhost: mosquitto.internal.grigri.cloud\nport: 8083\npath: /mqtt\nusername: vault:mosquitto/user#username\npassword: vault:mosquitto/user#password\n</code></pre> <p>Subscribe to all topics with <code>#</code>.</p>"},{"location":"user-guide/add-or-remove-nodes/","title":"Add or remove nodes","text":"<p>Or how to scale vertically. To replace the same node with a clean OS, remove it and add it again.</p>"},{"location":"user-guide/add-or-remove-nodes/#add-new-nodes","title":"Add new nodes","text":"<p>Tip</p> <p>You can add multiple nodes at the same time</p> <p>Add nodes details to the inventory at the end of the group (kube_control_plane or kube_node):</p> <pre><code>diff --git a/metal/inventories/master/inventory.ini b/metal/inventories/master/inventory.ini\nindex fe0ca8b..ddbca8d 100644\n--- a/metal/inventories/master/inventory.ini\n+++ b/metal/inventories/master/inventory.ini\n@@ -18,6 +18,7 @@ odroid-hc4\n k8s-odroid-hc4-1\n k8s-odroid-hc4-2\n k8s-odroid-hc4-3\n+k8s-odroid-hc4-4\n\n [amd64]\n k8s-amd64-1\n</code></pre> <p>Setup OS and network: manual Setup</p> <p>Join the cluster:</p> <pre><code>make metal\n</code></pre> <p>That's it!</p>"},{"location":"user-guide/add-or-remove-nodes/#remove-a-node","title":"Remove a node","text":"<p>Danger</p> <p>It is recommended to remove nodes one at a time</p> <p>Remove it from the inventory:</p> <pre><code>diff --git a/metal/inventories/master/inventory.ini b/metal/inventories/master/inventory.ini\nindex fe0ca8b..19891bf 100644\n--- a/metal/inventories/master/inventory.ini\n+++ b/metal/inventories/master/inventory.ini\n@@ -17,7 +17,6 @@ odroid-hc4\n [odroid-hc4]\n k8s-odroid-hc4-1\n k8s-odroid-hc4-2\n-k8s-odroid-hc4-3\n\n [amd64]\n k8s-amd64-1\n</code></pre> <p>Drain the node:</p> <pre><code>kubectl drain ${NODE_NAME} --delete-emptydir-data --ignore-daemonsets --force\n</code></pre> <p>Remove the node from the cluster</p> <pre><code>kubectl delete node ${NODE_NAME}\n</code></pre> <p>Shutdown the node:</p> <pre><code>ssh root@${NODE_IP} poweroff\n</code></pre>"},{"location":"user-guide/clone-data/","title":"Clone data between volumes","text":""},{"location":"user-guide/clone-data/#rsync","title":"rsync","text":"<p>No dependencies:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rsync\n  namespace: plex\nspec:\n  template:\n    metadata:\n      name: rsync\n    spec:\n      containers:\n        - name: rsync\n          image: instrumentisto/rsync-ssh\n          command:\n            - rsync\n            - -av\n            - --numeric-ids\n            - /src/\n            - /dest/\n          volumeMounts:\n            - name: src\n              mountPath: \"/src/\"\n            - name: dest\n              mountPath: \"/dest/\"\n      volumes:\n        - name: src\n          persistentVolumeClaim:\n            claimName: config-plex-0\n        - name: dest\n          persistentVolumeClaim:\n            claimName: config-plex-0-zfs\n      restartPolicy: Never\n      nodeSelector:\n        name: grigri\n</code></pre>"},{"location":"user-guide/cpu-optimize-tuning/","title":"CPU optimize tuning","text":""},{"location":"user-guide/cpu-optimize-tuning/#amd-ryzen-7950x","title":"AMD Ryzen 7950X","text":"<p>Go to BIOS and turn on overclocking and change <code>Advanced -&gt; AMD Overclocking -&gt; AMD Overclocking -&gt; Precision Boost Overdrive</code> from <code>auto</code> to advance. Then in <code>Curve Optimizer</code> select <code>all cores</code> and change sign from <code>positive</code> to <code>negative</code>. Start testing with a magnitude of 30 and if it is not stable go down.</p> <p>E.g. for testing CPU:</p> <pre><code>sysbench --threads=\"$(nproc)\" cpu run --cpu-max-prime=2000000000\n</code></pre> <p>Additionally, you can change <code>Platform Thermal Throttle Ctrl</code> to manual and set power limit to 85W. I didn't apply this.</p> <p>Ref: https://www.youtube.com/watch?v=FaOYYHNGlLs</p>"},{"location":"user-guide/expand-longhorn-volume/","title":"Expand Longhorn volume","text":"<p>Longhorn requires volumes to be detached in order to expand them.</p> <p>An example of volume expansion:</p> <pre><code>kubectl -n gitea delete --cascade=orphan sts gitea-postgres\nkubectl -n gitea delete pod gitea-postgres-0\nkubectl -n gitea patch pvc pgdata-gitea-postgres-0 -p '{ \"spec\": { \"resources\": { \"requests\": { \"storage\": \"1.5Gi\" }}}}'\n</code></pre>"},{"location":"user-guide/freshrss/","title":"FreshRSS","text":""},{"location":"user-guide/freshrss/#sqlite-backup","title":"SQLite Backup","text":""},{"location":"user-guide/freshrss/#export","title":"Export","text":"<p>To export data from FreshRSS using SQLite, utilize the following command:</p> <pre><code>./cli/export-sqlite-for-user.php --user ${USERNAME} --filename /tmp/freshrss.sqlite\n</code></pre>"},{"location":"user-guide/freshrss/#import","title":"Import","text":"<p>To import previously exported SQLite data back into FreshRSS, use the following command:</p> <pre><code>./cli/import-sqlite-for-user.php --user ${USERNAME} --force-overwrite --filename /tmp/freshrss.sqlite\n</code></pre>"},{"location":"user-guide/freshrss/#postgresql-configuration","title":"PostgreSQL Configuration","text":"<p>Optimizing Full-text search in PostgreSQL for FreshRSS involves adding indexes. This can significantly improve search performance without modifying FreshRSS' code (which uses ILIKE).</p> <p>First, ensure you have the <code>pg_trgm</code> extension installed:</p> <pre><code>CREATE EXTENSION pg_trgm;\n</code></pre> <p>Then, create the necessary indexes for title and content:</p> <pre><code>CREATE INDEX gin_trgm_index_title ON \"freshrss_entry\" USING gin(title gin_trgm_ops);\nCREATE INDEX gin_trgm_index_content ON \"freshrss_entry\" USING gin(content gin_trgm_ops);\n</code></pre> <p>Replace \"freshrss_entry\" with the appropriate entry name (e.g., freshrss_alice_entry).</p> <p>For faster searches on authors (e.g., author:Alice), add another index:</p> <pre><code>CREATE INDEX gin_trgm_index_author ON freshrss_entry USING gin(author gin_trgm_ops);\n</code></pre> <p>Repeat this process for other text fields as needed. Refer to the CREATE TABLE _entry section for the list of fields.</p>"},{"location":"user-guide/freshrss/#references","title":"References","text":"<ul> <li>FreshRSS Database Configuration</li> </ul>"},{"location":"user-guide/grigri/","title":"grigri","text":"<p>Motherboard: Supermicro A1SRi-2758F</p>"},{"location":"user-guide/grigri/#remote-access","title":"Remote access","text":"<p>Go to https://grigri-ipmi.grigri: <code>Remote Control -&gt; Console Redirection</code></p> <pre><code>sudo archlinux-java set java-8-openjdk\njavaws /tmp/launch.jnlp\n</code></pre>"},{"location":"user-guide/install-pre-commit-hooks/","title":"Install pre-commit hooks","text":"<p>Git hook scripts are useful for identifying simple issues before commiting changes.</p> <p>Install pre-commit first, one-liner for Arch users:</p> <pre><code>sudo pacman -S python-pre-commit\n</code></pre> <p>Then install git hook scripts:</p> <pre><code>make git-hooks\n</code></pre>"},{"location":"user-guide/kanidm/","title":"Kanidm","text":""},{"location":"user-guide/kanidm/#ldap-connection","title":"LDAP connection","text":"<p>For login with LDAP accounts your user has to have enabled the POSIX attributes and need to set a Unix password.</p>"},{"location":"user-guide/kanidm/#modify-acp","title":"Modify ACP","text":"<p>Disallow displayname self modification:</p> <pre><code>cat &lt;&lt; EOF&gt; /tmp/modify.json\n[\n    { \"removed\": [\"acp_modify_removedattr\", \"displayname\"] },\n    { \"removed\": [\"acp_modify_presentattr\", \"displayname\"] }\n]\nEOF\nkanidm raw modify '{\"eq\": [\"name\", \"idm_self_acp_write\"]}'  /tmp/modify.json\nkanidm raw search '{\"eq\": [\"name\", \"idm_self_acp_write\"]}'\n</code></pre>"},{"location":"user-guide/kanidm/#create-user","title":"Create user","text":"<pre><code>kanidm person create demo-user \"demo-user\" -D idm_admin\nkanidm person update demo-user --mail \"demo-user@example.com\" -D idm_admin\nkanidm person credential create-reset-token demo-user -D idm_admin\n\nkanidm group list -D idm_admin | rg name | rg users\nkanidm group add-members ${GROUP_NAME} demo-user -D idm_admin\n</code></pre>"},{"location":"user-guide/kanidm/#add-app-to-sso","title":"Add app to SSO","text":"<p>Example with Grafana:</p> <pre><code>kanidm system oauth2 create grafana grafana  https://grafana.grigri.cloud/login/generic_oauth -D admin\nkanidm group create grafana-users -D admin\nkanidm group add-members grafana-users ${USER} -D admin\nkanidm system oauth2 update-scope-map grafana grafana-users openid profile email -D admin\nkanidm system oauth2 show-basic-secret grafana -D admin\nkanidm group create grafana-admins -D admin\nkanidm group add-members grafana-admins ${USER} -D admin\nkanidm system oauth2 update-sup-scope-map grafana grafana-admins admin -D admin\nkanidm system oauth2 prefer-short-username grafana -D admin\nkanidm system oauth2 set-landing-url grafana https://grafana.grigri.cloud/login/generic_oauth\n</code></pre> <p>If PKCE needs to be disabled:</p> <pre><code>kanidm system oauth2 warning-insecure-client-disable-pkce ${CLIENT}\n</code></pre>"},{"location":"user-guide/kanidm/#use-groups-in-sso","title":"Use groups in SSO","text":"<p>To pass groups in JWT you need to ask for <code>openid groups</code> scopes.</p> <p>After next login, you will receive groups <code>uuid</code> and groups <code>spn</code> in the token:</p> <pre><code>    ...\n  \"scopes\": [\n    \"email\",\n    \"groups\",\n    \"openid\",\n    \"profile\"\n  ],\n  \"groups\": [\n    \"idm_all_persons@idm.grigri.cloud\",\n    \"00000000-0000-0000-0000-000000000035\",\n    \"idm_all_accounts@idm.grigri.cloud\",\n    \"00000000-0000-0000-0000-000000000036\",\n    \"XXXXXX@idm.grigri.cloud\",\n    \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\",\n    ...\n  ]\n}\n</code></pre> <p>Note: When groups scope is activated your header size will be above 4k so you will need to add this annotation to your ingress:</p> <pre><code>nginx.ingress.kubernetes.io/proxy-buffer-size: \"16k\"\n</code></pre>"},{"location":"user-guide/kanidm/#upgrade","title":"Upgrade","text":"<p>Check if upgrade is possible:</p> <pre><code>kanidmd domain upgrade-check\n</code></pre>"},{"location":"user-guide/kanidm/#disable-anonymous-access","title":"Disable anonymous access","text":"<pre><code>kanidm service-account validity expire-at anonymous '2024-11-14T00:00:00+01:00'\n</code></pre>"},{"location":"user-guide/kanidm/#increase-expiration-time","title":"Increase expiration time","text":"<pre><code>kanidm group account-policy auth-expiry idm_all_accounts 2592000\n</code></pre>"},{"location":"user-guide/kanidm/#service-account-permissions-to-validate-credentials","title":"Service account permissions to validate credentials","text":"<pre><code>kanidm group add-members idm_people_pii_read\n</code></pre>"},{"location":"user-guide/kubernetes-upgrade/","title":"Kubernetes upgrade","text":"<p>Update <code>k3s_version</code> to desired version and then run:</p> <pre><code>cd metal\nANSIBLE_EXTRA_ARGS=\"-t k3s-upgrade -e serial=1\" make cluster\n</code></pre> <p>Note: it worked perfectly with serial=100% or running it by default.</p>"},{"location":"user-guide/ldap/","title":"LDAP","text":""},{"location":"user-guide/ldap/#deployment","title":"Deployment","text":"<p>The deployment of the Lightweight Directory Access Protocol (LDAP) application was carried out in k8s using the Fedora guide for OpenShift as reference.</p>"},{"location":"user-guide/ldap/#migration","title":"Migration","text":"<p>The migration process involved recreating the backend and suffix, as follows:</p> <pre><code>dsconf localhost backend create --suffix dc=grigri,dc=cloud --be-name userroot --create-suffix\n</code></pre> <p>To import current values, LDIF files were utilized. First, the current LDIF files were exported using the following command:</p> <pre><code>ldapsearch -w 'XXXXXXX' -D 'cn=Directory Manager' -H ldaps://ldap.grigri -b dc=grigri,dc=cloud -LLL \"(objectclass=*)\"\n</code></pre> <p>Next, Apache Directory Studio was used for importing the exported files.</p>"},{"location":"user-guide/ldap/#permissions","title":"Permissions","text":"<p>The permissions for the LDAP deployment were set in accordance with the guidelines provided by Red Hat's official documentation. The following commands were executed:</p> <pre><code>ldapmodify -w 'XXXXXXX' -D 'cn=Directory Manager' -H ldaps://ldap.grigri -b dc=grigri,dc=cloud -x\ndn: dc=grigri,dc=cloud\nchangetype: modify\nadd: aci\naci: (targetattr!=\"userPassword || aci\")(version 3.0; acl \"Enable anonymous ac\n cess\"; allow (read, search, compare) userdn=\"ldap:///anyone\";)\n\nldapmodify -w 'XXXXXXX' -D 'cn=Directory Manager' -H ldaps://ldap.grigri -b dc=grigri,dc=cloud -x\ndn: ou=People,dc=grigri,dc=cloud\nchangetype: modify\nadd: aci\naci: (targetattr=\"userpassword\")(version 3.0; acl \"read password access\"; allo\n w(read) userdn =  \"ldap:///uid=*,ou=Read,ou=ServiceUsers,dc=grigri,dc=cloud\";\n )\n</code></pre>"},{"location":"user-guide/ldap/#references","title":"References","text":""},{"location":"user-guide/migrate_controller_node/","title":"Migrate controller node","text":"<p>Based in K3s backup and restore doc.</p>"},{"location":"user-guide/migrate_controller_node/#procedure","title":"Procedure","text":"<ol> <li>Take etcd snapshot and stop k3s in old controller.</li> <li>Copy <code>/var/lib/rancher/k3s/server</code> and etcd snapshot to new controller.</li> <li>Update Ansible inventory controller node and start new controller.</li> <li>Check API is working and there are no agents.</li> <li>Start all agents with <code>make cluster</code>.</li> </ol> <p>Example:</p> <pre><code># k8s-amd64-1\nk3s etcd-snapshot save\nsystemctl stop k3s\n\nrsync -av --delete /var/lib/rancher/k3s/server/ backup@prusik:/datasets/backups/k3s/server\n\n# grigri\nk3s server --cluster-reset --cluster-reset-restore-path=/datasets/backups/k3s/server/db/snapshots/on-demand-grigri-1716963887 --token /datasets/backups/k3s/server/token\n\n# change inventory with new controller and move controller to worker\n\n# localhost\ncd metal\nANSIBLE_EXTRA_ARGS=\"--limit prusik\" make cluster\nkubectl get nodes\nmake cluster\n\n# remove labels\n</code></pre>"},{"location":"user-guide/move_data/","title":"Move data","text":""},{"location":"user-guide/move_data/#import-zfs-dataset","title":"Import ZFS dataset","text":""},{"location":"user-guide/move_data/#steps","title":"Steps","text":"<ul> <li>stop app</li> <li>clone volume</li> <li>create new volume</li> <li>change volume for app</li> <li>check app</li> <li>remove old volume</li> </ul>"},{"location":"user-guide/move_data/#commands","title":"Commands","text":"<p>In ZFS server:</p> <pre><code># variables\nPVC=datasets/k8s/l/v/pvc-0f17e0bf-6741-44fa-9e37-e5ed394ff56b\nNAME=transcoder-rabbit\nSIZE=1G\nNAMESPACE=transcoder\n\nPVC_NAME=${NAME}\nDATASET=datasets/openebs\nNEW_DATASET=${DATASET}/${NAME}\n\nzfs snapshot ${PVC}@clone\nzfs clone ${PVC}@clone ${NEW_DATASET}\nzfs promote ${NEW_DATASET}\nzfs unmount ${NEW_DATASET}\nzfs set mountpoint=legacy ${NEW_DATASET}\nzfs set quota=${SIZE} ${NEW_DATASET}\n\ncat &lt;&lt; EOF &gt; /tmp/pv.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: ${NAME}\nspec:\n  accessModes:\n  - ReadWriteOnce\n  capacity:\n    storage: ${SIZE}i # size of the volume\n  claimRef:\n    apiVersion: v1\n    kind: PersistentVolumeClaim\n    name: ${PVC_NAME}\n    namespace: ${NAMESPACE}\n  csi:\n    driver: zfs.csi.openebs.io\n    fsType: zfs\n    volumeAttributes:\n      openebs.io/poolname: ${DATASET}\n    volumeHandle: ${NAME}\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - grigri\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: openebs-zfspv\n  volumeMode: Filesystem\nEOF\nk3s kubectl apply -f /tmp/pv.yaml\n\ncat &lt;&lt; EOF &gt; /tmp/zfs-volume.yaml\napiVersion: zfs.openebs.io/v1\nkind: ZFSVolume\nmetadata:\n  finalizers:\n  - zfs.openebs.io/finalizer\n  name: ${NAME}\n  namespace: zfs-localpv\nspec:\n  capacity: \"$(echo \"(${SIZE::-1} * 1024 * 1024 * 1024) / 1\" | bc)\" # size of the volume in bytes\n  fsType: zfs\n  ownerNodeID: grigri\n  shared: \"yes\"\n  poolName: ${DATASET}\n  volumeType: DATASET\nstatus:\n  state: Ready\nEOF\nk3s kubectl apply -f /tmp/zfs-volume.yaml\n</code></pre>"},{"location":"user-guide/move_data/#move-zfs-volume-between-nodes","title":"Move ZFS volume between nodes","text":""},{"location":"user-guide/move_data/#steps_1","title":"Steps","text":"<ul> <li>Stop app</li> <li>Send snapshots</li> <li>Create new volume</li> <li>Change volume for app</li> <li>Check app</li> <li>Remove old volume</li> </ul> <p>I'm not sure how to handle PVC Bound. At the moment I just marked the PV as Retain and recreate the PVC manually.</p>"},{"location":"user-guide/move_data/#commands_1","title":"Commands","text":"<p>In ZFS server:</p> <pre><code># variables\nPVC_NAME=minio-backup\nSIZE=2000G\nNAMESPACE=minio\nNODE_NAME=grigri\n\nDATASET=datasets/openebs\nNEW_DATASET=${DATASET}/${PVC_NAME}\n\nzfs set mountpoint=legacy ${NEW_DATASET}\nzfs set quota=${SIZE} ${NEW_DATASET}\n\ncat &lt;&lt; EOF &gt; /tmp/pv.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: ${PVC_NAME}\nspec:\n  accessModes:\n  - ReadWriteOnce\n  capacity:\n    storage: ${SIZE}i # size of the volume\n  claimRef:\n    apiVersion: v1\n    kind: PersistentVolumeClaim\n    name: ${PVC_NAME}\n    namespace: ${NAMESPACE}\n  csi:\n    driver: zfs.csi.openebs.io\n    fsType: zfs\n    volumeAttributes:\n      openebs.io/poolname: ${DATASET}\n    volumeHandle: ${PVC_NAME}\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - ${NODE_NAME}\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: openebs-zfspv\n  volumeMode: Filesystem\nEOF\nk3s kubectl apply -f /tmp/pv.yaml\n\ncat &lt;&lt; EOF &gt; /tmp/zfs-volume.yaml\napiVersion: zfs.openebs.io/v1\nkind: ZFSVolume\nmetadata:\n  finalizers:\n  - zfs.openebs.io/finalizer\n  name: ${PVC_NAME}\n  namespace: zfs-localpv\nspec:\n  capacity: \"$(echo \"(${SIZE::-1} * 1024 * 1024 * 1024) / 1\" | bc)\" # size of the volume in bytes\n  fsType: zfs\n  ownerNodeID: ${NODE_NAME}\n  shared: \"yes\"\n  poolName: ${DATASET}\n  volumeType: DATASET\nstatus:\n  state: Ready\nEOF\nk3s kubectl apply -f /tmp/zfs-volume.yaml\n</code></pre>"},{"location":"user-guide/navidrome/","title":"Navidrome","text":""},{"location":"user-guide/navidrome/#library","title":"Library","text":"<p>We have two different libraries:</p> <ul> <li>Main library: <code>/music/</code> (mounted from <code>navidrome-music</code> PVC)</li> <li>Lidarr library: <code>/music/lidarr/</code> (mounted from <code>/datasets/music</code> on the host)</li> </ul> <p>Main library is backuped by Velero. Lidarr library is not backed up. If you want to move something from Lidarr library to main library, you need to copy files from <code>/music/lidarr/</code> to <code>/music/</code> in the proper directory and adjust the metadata accordingly.</p> <p>Note: Lidarr does not know about files in the main library.</p>"},{"location":"user-guide/navidrome/#update-music-metadata","title":"Update music metadata","text":"<pre><code># Explore pod\nkubectl --context=grigri -n navidrome exec -it $(kubectl -n navidrome get pod -l app.kubernetes.io/name=navidrome -o jsonpath=\"{.items[0].metadata.name}\") -- /bin/sh\n\n# Copy files to local\nkubectl --context=grigri -n navidrome cp $(kubectl -n navidrome get pod -l app.kubernetes.io/name=navidrome -o jsonpath=\"{.items[0].metadata.name}\"):/music/path/ /tmp/path/\n\n# Change metadata with local tools\neasytag /tmp/path/\n\n# Copy files back to pod\nkubectl --context=grigri -n navidrome cp /tmp/path/ $(kubectl -n navidrome get pod -l app.kubernetes.io/name=navidrome -o jsonpath=\"{.items[0].metadata.name}\"):/music/path/\n</code></pre> <p>After updating the metadata, you may need to rescan the music library from the Navidrome web interface and remove missing files: https://navidrome.grigri.cloud/app/#/missing.</p>"},{"location":"user-guide/nextcloud/","title":"Nextcloud","text":"<ul> <li>Migrate auth to OIDC</li> <li>Migrate Nextcloud</li> <li>Nextcloud<ul> <li>apps</li> <li>data</li> </ul> </li> <li>Postgres</li> <li>occ command</li> <li>Troubleshooting</li> <li>Not automatically upgraded</li> <li>Automatically upgrades fails</li> <li>Enable other auth methods</li> </ul>"},{"location":"user-guide/nextcloud/#migrate-auth-to-oidc","title":"Migrate auth to OIDC","text":"<ul> <li>Configure Kanidm oauth2 with   OpenID Connect user backend for Nextcloud.</li> <li>Follow Kanidm guide for   Nextcloud config.</li> <li>Use<code>displayname</code> from Kanidm (<code>name</code> in OIDC JWT) as user ID and keep user IDs from LDAP auth.</li> </ul>"},{"location":"user-guide/nextcloud/#migrate-nextcloud","title":"Migrate Nextcloud","text":""},{"location":"user-guide/nextcloud/#nextcloud_1","title":"Nextcloud","text":""},{"location":"user-guide/nextcloud/#apps","title":"apps","text":"<pre><code>POD_NAME=$(KUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud get pod -l app.kubernetes.io/component=app --no-headers -o custom-columns=\":metadata.name\")\nKUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud cp config/www/nextcloud/apps/ $POD_NAME:/tmp/\n\nKUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud exec -it $POD_NAME -- bash\nrm -rf /var/www/html/apps\nmv /tmp/apps /var/www/html/apps\nchown -R www-data:www-data /var/www/html/apps\n</code></pre>"},{"location":"user-guide/nextcloud/#data","title":"data","text":"<pre><code>POD_NAME=$(KUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud get pod -l app.kubernetes.io/component=app --no-headers -o custom-columns=\":metadata.name\")\nfor DIR in $(find /datasets/nextcloud/ -maxdepth 1 -mindepth 1 -not -path '*/teresa' -not -path '*/pando' -not -path '*/billee'); do\n    echo $DIR;\n    KUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud cp --retries=5 $DIR $POD_NAME:/var/www/html/data/\ndone\n#KUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud cp --retries=5 /datasets/fotos $POD_NAME:/var/www/html/data/pando/files/\nKUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud exec -it $POD_NAME -- bash\nchown -R www-data:www-data /var/www/html/data\n</code></pre> <p>Alternative using rsync:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rsync\n  namespace: nextcloud\nspec:\n  template:\n    metadata:\n      name: rsync\n    spec:\n      containers:\n        - name: rsync\n          image: instrumentisto/rsync-ssh\n          command:\n            - rsync\n            - -avz\n            - --numeric-ids\n            - --delete\n            - --progress\n            - /src/\n            - /dest/data/\n          volumeMounts:\n            - name: src\n              mountPath: \"/src/\"\n            - name: dest\n              mountPath: \"/dest/\"\n      volumes:\n        - name: src\n          hostPath:\n            path: /datasets/nextcloud/\n            type: Directory\n        - name: dest\n          persistentVolumeClaim:\n            claimName: nextcloud-nextcloud-data\n      restartPolicy: Never\n      nodeSelector:\n        name: grigri\n</code></pre>"},{"location":"user-guide/nextcloud/#postgres","title":"Postgres","text":"<pre><code>docker exec -it nextcloud_db_1 bash\n\nPGPASSWORD=\"$POSTGRES_PASSWORD\" pg_dump \"$POSTGRES_DB\" -h localhost -U \"$POSTGRES_USER\" -f /tmp/nextcloud-sqlbkp.bak\nexit\ndocker cp nextcloud_db_1:/tmp/nextcloud-sqlbkp.bak /tmp/\nKUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud cp /tmp/nextcloud-sqlbkp.bak nextcloud-postgres-0:/tmp/\n\nKUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud exec -it nextcloud-postgres-0 -- bash\nsu - postgres\npsql -d template1 -c \"DROP DATABASE \\\"nextcloud\\\";\"\npsql -d template1 -c \"CREATE DATABASE \\\"nextcloud\\\";\"\npsql -d nextcloud -f /tmp/nextcloud-sqlbkp.bak\n</code></pre>"},{"location":"user-guide/nextcloud/#occ-command","title":"occ command","text":"<p>Run with kubectl:</p> <pre><code>chsh -s /bin/bash www-data\nsu - www-data\n/var/www/html/occ\n</code></pre>"},{"location":"user-guide/nextcloud/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/nextcloud/#not-automatically-upgraded","title":"Not automatically upgraded","text":"<pre><code>chsh -s /bin/bash www-data\nsu - www-data\n/var/www/html/occ upgrade\n</code></pre>"},{"location":"user-guide/nextcloud/#automatically-upgrades-fails","title":"Automatically upgrades fails","text":"<p>Check <code>https://nextcloud.grigri.cloud/index.php/settings/integrity/failed</code>.</p> <p>Always fails in code integrity check without any errors, just hangs forever in the web browser auto updater.</p> <pre><code>chsh -s /bin/bash www-data\nsu - www-data\n/var/www/html/occ maintenance:mode\n</code></pre>"},{"location":"user-guide/nextcloud/#enable-other-auth-methods","title":"Enable other auth methods","text":"<pre><code>chsh -s /bin/bash www-data\nsu - www-data\n/var/www/html/occ config:app:set --value=0 user_oidc allow_multiple_user_backends\n</code></pre>"},{"location":"user-guide/restore-backup/","title":"Restore backup","text":""},{"location":"user-guide/restore-backup/#zfs","title":"ZFS","text":"<pre><code>zfs rollback ${ZFS_VOLUME}@{ZFS_SNAPSHOT}\n</code></pre>"},{"location":"user-guide/restore-backup/#postgres","title":"Postgres","text":""},{"location":"user-guide/restore-backup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/restore-backup/#exceeds-maximum-replication-lag","title":"Exceeds maximum replication lag","text":"<p>If after restore there are this kind of errors:</p> <p>My wal position exceeds maximum replication lag</p> <p>Replica can be promoted to leader using <code>patronictl failover</code> command as root.</p>"},{"location":"user-guide/restore-backup/#operator-could-not-connect-to-database","title":"Operator could not connect to database","text":"<p>Update passwords using values in secrets. Inside a container:</p> <pre><code>psql -U postgres\nALTER USER postgres WITH PASSWORD 'VALUE_FROM_SECRET';\nALTER USER standby WITH PASSWORD 'VALUE_FROM_SECRET';\nALTER USER APP_USER_NAME WITH PASSWORD 'VALUE_FROM_SECRET';\n</code></pre>"},{"location":"user-guide/restore-backup/#velero","title":"Velero","text":""},{"location":"user-guide/restore-backup/#steps","title":"Steps","text":"<ul> <li>Remove nodeSelector</li> <li>Create backup</li> <li>Change old PV <code>persistentVolumeReclaimPolicy</code> to <code>Retain</code></li> <li>Stop service (deployment and destroy PVCs). Recommended manually: scale argocd-repo-server to 0 and proceed manually.</li> <li>Restore from backup</li> <li>Enable service. Scale up argocd-repo-server.</li> <li>Check everything is OK</li> <li>Remove old PV</li> </ul>"},{"location":"user-guide/restore-backup/#restoring-to-a-different-node","title":"Restoring to a Different Node","text":"<p>To restore to a different node, create a ConfigMap with the following YAML:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  # any name can be used; Velero uses the labels (below)\n  # to identify it rather than the name\n  name: change-pvc-node-selector-config\n  # must be in the velero namespace\n  namespace: velero\n  # the below labels should be used verbatim in your\n  # ConfigMap.\n  labels:\n    # this value-less label identifies the ConfigMap as\n    # config for a plugin (i.e. the built-in restore item action plugin)\n    velero.io/plugin-config: \"\"\n    # this label identifies the name and kind of plugin\n    # that this ConfigMap is for.\n    velero.io/change-pvc-node-selector: RestoreItemAction\ndata:\n  # add 1+ key-value pairs here, where the key is the old\n  # node name and the value is the new node name.\n  grigri: prusik\n</code></pre> <p>Warning: ArgoCD labels are going to be restored too. If you are doing the restore into other namespace as in the example, disable ArgoCD or modify the labels.</p>"},{"location":"user-guide/restore-backup/#restoring-with-a-different-storage-class","title":"Restoring with a Different Storage Class","text":"<p>To restore with a different storage class, create a ConfigMap with the following YAML:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  # any name can be used; Velero uses the labels (below)\n  # to identify it rather than the name\n  name: change-storage-class-config\n  # must be in the velero namespace\n  namespace: velero\n  # the below labels should be used verbatim in your\n  # ConfigMap.\n  labels:\n    # this value-less label identifies the ConfigMap as\n    # config for a plugin (i.e. the built-in restore item action plugin)\n    velero.io/plugin-config: \"\"\n    # this label identifies the name and kind of plugin\n    # that this ConfigMap is for.\n    velero.io/change-storage-class: RestoreItemAction\ndata:\n  # add 1+ key-value pairs here, where the key is the old\n  # storage class name and the value is the new storage\n  # class name.\n  openebs-zfspv: backup\n</code></pre> <p>Warning: Ensure that the original parent zfs volume exists in the target storage class (e.g., <code>datasets/openebs</code>).</p> <pre><code>velero restore create --from-backup ${BACKUP_NAME} --include-namespaces ${NAMESPACE} --restore-volumes=true --namespace-mappings ${NAMESPACE}:${TARGET_NAMESPACE}\n</code></pre> <p>Note: ArgoCD labels will also be restored. If restoring to a different namespace, consider disabling ArgoCD or modifying the labels accordingly.</p> <p>Example filtering by resource and labels:</p> <pre><code>velero restore create --from-backup retain-quaterly-20240910023040  --include-namespaces mintpsicologia --restore-volumes=true --namespace-mappings mintpsicologia:mintpsicologia --include-resources persistentvolumes,persistentvolumeclaims --selector app.kubernetes.io/name=mariadb\n</code></pre>"},{"location":"user-guide/run-commands-on-multiple-nodes/","title":"Run commands on multiple nodes","text":"<p>Use ansible-console:</p> <pre><code>cd metal\nmake console\n</code></pre> <p>Then enter the command(s) you want to run.</p> <p>Example</p> <p><code>root@all (4)[f:5]$ uptime</code></p> <pre><code>metal0 | CHANGED | rc=0 &gt;&gt;\n 10:52:02 up 2 min,  1 user,  load average: 0.17, 0.15, 0.06\nmetal1 | CHANGED | rc=0 &gt;&gt;\n 10:52:02 up 2 min,  1 user,  load average: 0.14, 0.11, 0.04\nmetal3 | CHANGED | rc=0 &gt;&gt;\n 10:52:02 up 2 min,  1 user,  load average: 0.03, 0.02, 0.00\nmetal2 | CHANGED | rc=0 &gt;&gt;\n 10:52:02 up 2 min,  1 user,  load average: 0.06, 0.06, 0.02\n</code></pre>"},{"location":"user-guide/upgrades/","title":"Upgrades","text":""},{"location":"user-guide/upgrades/#os-upgrades","title":"OS upgrades","text":"<p>Managed by <code>unattended-upgrade</code> in Debian based distributions and rebooted by <code>kured</code> when needed.</p> <p>Review the update history in <code>/var/log/unattended-upgrades/unattended-upgrades-dpkg.log</code></p>"},{"location":"user-guide/upgrades/#k3s-upgrades","title":"k3s upgrades","text":"<p>Managed by system-upgrade-controller. Increase K3s version in <code>system/system-upgrade/k3s/kustomization.yaml</code> file.</p>"},{"location":"user-guide/valetudo/","title":"Valetudo","text":""},{"location":"user-guide/valetudo/#valetudo-upgrade","title":"Valetudo upgrade","text":"<pre><code>ssh root@tanque.iot.grigri\nkillall valetudo\nwget https://github.com/Hypfer/Valetudo/releases/latest/download/valetudo-aarch64 -O /data/valetudo\nreboot\n</code></pre>"},{"location":"user-guide/valetudo/#firmware-upgrade","title":"Firmware upgrade","text":"<p>Note: Dreame doesn't provide any firmware update changelog or information about the new firmware.</p> <p>Check if there are new firmware versions available on the Dreame L10S Ultra Dustbuilder.</p>"},{"location":"user-guide/valetudo/#backup","title":"Backup","text":"<pre><code>ssh root@tanque.iot.grigri\ndd if=/dev/by-name/private | gzip -9 &gt; /tmp/backup_private.dd.gz\ndd if=/dev/by-name/misc | gzip -9 &gt; /tmp/backup_misc.dd.gz\ntar -czvf /tmp/backup_mnt.tar.gz /mnt\nexit\nscp -r -O root@tanque.iot.grigri:/tmp/backup_private.dd.gz .\nscp -r -O root@tanque.iot.grigri:/tmp/backup_misc.dd.gz .\nscp -r -O root@tanque.iot.grigri:/tmp/backup_mnt.tar.gz .\n</code></pre>"},{"location":"user-guide/valetudo/#build-firmware","title":"Build firmware","text":"<p>Get the firmware from the dustbuilder and download it to the robot.</p> <p>Go to Dreame L10S Ultra section.</p> <p>Email: <code>anonymous</code></p> <p>Authorized_keys: <code>pass personal/authorized_keys</code></p> <p>Note: Just the first line is used.</p> <p>Get serial number and config value from <code>pass iot/tanque_firmware</code>.</p> <p>Note: config value from fastboot doesn't change.</p>"},{"location":"user-guide/valetudo/#install","title":"Install","text":"<p>You need to have your robot already rooted to use this firmware! The robot needs to be in its docking station and fully charged!</p> <ol> <li>Connect to robot via SSH using your SSH key</li> </ol> <pre><code>ssh root@tanque.iot.grigri\n</code></pre> <ol> <li>Download firmware update to <code>/tmp</code></li> </ol> <pre><code>cd /tmp\nwget --no-check-certificate {url-of-firmware.tar.gz}\n</code></pre> <ol> <li>Unpack firmware package</li> </ol> <pre><code>tar -xzvf {name-of-firmware.tar.gz}\n</code></pre> <ol> <li>Run installer</li> </ol> <pre><code>./install.sh\n</code></pre> <p>The robot should install the firmware and reboot. This steps will update the Kernel, Rootfs and MCU firmware</p> <p>Note: It takes like 3 minutes to install and reboot since <code>{\"ret\":\"ok\"}</code> is shown.</p>"},{"location":"user-guide/valetudo/#vacuum-stream","title":"Vacuum stream","text":"<p>To access the vacuum camera you can install and run this project: vacuumstream.</p>"},{"location":"user-guide/valetudo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/valetudo/#text-file-busy-error","title":"\"Text file busy\" error","text":"<p>Valetudo is still running. Try to kill it again.</p> <p>If the issue still occurs, delete the old binary before uploading the new one.</p>"},{"location":"user-guide/vault/","title":"Vault Migration: File to Raft Storage","text":"<p>Step-by-step guide to migrate Vault from file storage to raft storage using <code>vault operator migrate</code>.</p>"},{"location":"user-guide/vault/#overview","title":"Overview","text":"<p>The <code>vault operator migrate</code> command converts data in-place from file to raft format, preserving everything:</p> <ul> <li>KV secrets</li> <li>PKI root CA keys</li> <li>Auth methods</li> <li>Policies</li> </ul>"},{"location":"user-guide/vault/#1-backup-data","title":"1. Backup Data","text":"<p>Stop Vault and backup the PVC data (guarantees consistency):</p> <pre><code># Scale down vault to stop writes\nkubectl --context=grigri -n vault scale statefulset vault --replicas=0\n\n# Wait for pod to terminate\nkubectl --context=grigri -n vault wait --for=delete pod/vault-0 --timeout=120s\n\n# Create backup pod mounting the PVC\ncat &lt;&lt;EOF | kubectl --context=grigri apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vault-backup\n  namespace: vault\nspec:\n  containers:\n  - name: backup\n    image: busybox\n    args: [sleep, \"1000000\"]\n    volumeMounts:\n    - name: vault-data\n      mountPath: /data-source\n  volumes:\n  - name: vault-data\n    persistentVolumeClaim:\n      claimName: vault-file-vault-0\nEOF\n\n# Wait for backup pod\nkubectl --context=grigri -n vault wait --for=condition=ready pod/vault-backup --timeout=60s\n\n# Create backup tarball\nkubectl --context=grigri -n vault exec vault-backup -- tar czf /tmp/vault-backup.tar.gz -C /data-source .\n\n# Copy backup to local machine\nkubectl --context=grigri -n vault cp vault-backup:/tmp/vault-backup.tar.gz ./vault-backup.tar.gz\n\n# Cleanup backup pod\nkubectl --context=grigri -n vault delete pod vault-backup\n</code></pre>"},{"location":"user-guide/vault/#2-migrate-data-format","title":"2. Migrate Data Format","text":"<p>Start Vault temporarily to run the migration:</p> <pre><code># Scale vault back up\nkubectl --context=grigri -n vault scale statefulset vault --replicas=1\nkubectl --context=grigri -n vault wait --for=condition=ready pod/vault-0 --timeout=120s\n\n# Get root token\nTOKEN=$(kubectl --context=grigri -n vault get secret vault-unseal-keys -o jsonpath='{.data.vault-root}' | base64 -d)\n\n# Create migration config and run migration\nkubectl --context=grigri -n vault exec vault-0 -- sh -c \"\n  cat &gt; /tmp/migrate.hcl &lt;&lt; 'EOF'\nstorage_source \\\"file\\\" {\n  path = \\\"/vault/file\\\"\n}\nstorage_destination \\\"raft\\\" {\n  path = \\\"/vault/file\\\"\n  node_id = \\\"vault-0\\\"\n}\ncluster_addr = \\\"https://vault.vault:8201\\\"\nEOF\n\n  VAULT_SKIP_VERIFY=true VAULT_TOKEN=$TOKEN \\\n  vault operator migrate -config=/tmp/migrate.hcl\n\"\n</code></pre> <p>Expected output:</p> <pre><code>Migration successfully completed!\n</code></pre>"},{"location":"user-guide/vault/#3-update-vault-configuration","title":"3. Update Vault Configuration","text":"<p>Edit <code>platform/vault/templates/vault.yaml</code>:</p> <pre><code>config:\n  storage:\n    raft:\n      path: \"${ .Env.VAULT_STORAGE_FILE }\"\n      node_id: \"vault-0\"\n  listener:\n    tcp:\n      address: \"0.0.0.0:8200\"\n      tls_cert_file: /vault/tls/server.crt\n      tls_key_file: /vault/tls/server.key\n      cluster_address: \"0.0.0.0:8201\"\n  api_addr: \"https://vault.vault:8200\"\n  cluster_addr: \"https://vault.vault:8201\"\n  ui: true\n</code></pre> <p>Commit and push (ArgoCD will sync):</p> <pre><code>git add platform/vault/templates/vault.yaml\ngit commit -m \"feat(vault): migrate from file to raft storage\"\ngit push\n</code></pre>"},{"location":"user-guide/vault/#4-redeploy-vault","title":"4. Redeploy Vault","text":"<pre><code># Delete pod to pick up new config\nkubectl --context=grigri -n vault delete pod vault-0\n\n# Wait for vault to be ready\nkubectl --context=grigri -n vault wait --for=condition=ready pod/vault-0 --timeout=120s\n</code></pre>"},{"location":"user-guide/vault/#5-verify-migration","title":"5. Verify Migration","text":"<pre><code>TOKEN=$(kubectl --context=grigri -n vault get secret vault-unseal-keys -o jsonpath='{.data.vault-root}' | base64 -d)\n\nkubectl --context=grigri -n vault exec vault-0 -- sh -c \"\n  export VAULT_SKIP_VERIFY=true VAULT_TOKEN=$TOKEN\n\n  echo '=== Vault Status ==='\n  vault status\n\n  echo ''\n  echo '=== KV Secrets ==='\n  vault kv list secret/\n\n  echo ''\n  echo '=== PKI Mounts ==='\n  vault secrets list | grep pki\n\n  echo ''\n  echo '=== Raft Snapshot Test ==='\n  vault operator raft snapshot save /tmp/test.snap &amp;&amp; echo 'Raft snapshot OK'\n\"\n</code></pre> <p>Expected status output:</p> <pre><code>Storage Type             raft\nHA Enabled               true\nRaft Committed Index     XXX\nRaft Applied Index       XXX\n</code></pre>"},{"location":"user-guide/vault/#argocd-considerations","title":"ArgoCD Considerations","text":"<p>Vault modifies pod labels dynamically (<code>vault-active</code>, <code>vault-sealed</code>, etc.). ArgoCD may try to sync these back. Add to your Application manifest:</p> <pre><code>spec:\n  ignoreDifferences:\n    - kind: Pod\n      name: vault-0\n      jsonPointers:\n        - /metadata/labels/vault-active\n        - /metadata/labels/vault-sealed\n        - /metadata/labels/vault-initialized\n</code></pre>"},{"location":"user-guide/vault/#quick-reference","title":"Quick Reference","text":"<pre><code># Get root token\nkubectl --context=grigri -n vault get secret vault-unseal-keys -o jsonpath='{.data.vault-root}' | base64 -d\n\n# Check Vault status\nkubectl --context=grigri -n vault exec vault-0 -- vault status\n\n# Create raft snapshot\nkubectl --context=grigri -n vault exec vault-0 -- sh -c \"\n  VAULT_SKIP_VERIFY=true VAULT_TOKEN=&lt;token&gt; \\\n  vault operator raft snapshot save /vault/file/backup.snap\n\"\n\n# Restore raft snapshot\nkubectl --context=grigri -n vault exec vault-0 -- sh -c \"\n  VAULT_SKIP_VERIFY=true VAULT_TOKEN=&lt;token&gt; \\\n  vault operator raft snapshot restore /vault/file/backup.snap\n\"\n\n# Force leader step-down (for HA)\nkubectl --context=grigri -n vault exec vault-0 -- sh -c \"\n  VAULT_SKIP_VERIFY=true VAULT_TOKEN=&lt;token&gt; vault operator step-down\n\"\n</code></pre>"},{"location":"user-guide/vpn/","title":"VPN","text":"<p>Add a new user to the VPN group to allow them to connect to the VPN.</p> <pre><code>export USER=\nkanidm group add-members vpn-users ${USER}\nkanidm person posix set ${USER}\n</code></pre> <p>You can then download the VPN configuration file from the web interface at <code>https://pfsense.grigri/vpn_openvpn_export.php</code>.</p>"},{"location":"user-guide/wallabag/","title":"Wallabag","text":""},{"location":"user-guide/wallabag/#fix-user-login-fail","title":"Fix user login fail","text":"<pre><code>cd /var/www/wallabag\nsu -c \"php bin/console cache:clear --env=prod\" -s /bin/sh nobody\nsu -c \"php bin/console doctrine:migrations:migrate --no-interaction --env=prod\" -s /bin/sh nobody\n</code></pre>"},{"location":"user-guide/wan-phone-failover/","title":"WAN phone failover","text":"<p>Use phone as WAN failover mechanism.</p>"},{"location":"user-guide/wan-phone-failover/#steps","title":"Steps","text":"<p>Connect phone to router. In the phone:</p> <pre><code>Ajustes -&gt; Punto de acces port\u00e1til -&gt; Anclaje USB\n</code></pre> <p>In the router:</p> <pre><code>Interfaces -&gt; Add ue8 -&gt; DHCP\nSystem -&gt; Routing ...\n</code></pre>"}]}