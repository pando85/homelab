defaultRules:
  rules:
    etcd: false
    # Disable until fixing: https://github.com/prometheus-community/helm-charts/issues/1283
    kubeApiserve: false
    kubeProxy: false

alertmanager:
  alertmanagerSpec:
    replicas: 2

    useExistingSecret: true

    configMaps:
      - alertmanager-telegram-template

    configSecret: alertmanager-secret

    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: alertmanager

  ingress:
    enabled: true
    ingressClassName: nginx-internal
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod-dns
      external-dns.alpha.kubernetes.io/enabled: "true"
    hosts:
      - alertmanager.internal.grigri.cloud
    paths:
      - /
    tls:
      - secretName: alertmanager-general-tls
        hosts:
          - alertmanager.internal.grigri.cloud


grafana:
  enabled: true

  defaultDashboardsTimezone: cet

  # needed for loading initial datasources
  admin:
    existingSecret: grafana-admin-secret
    userKey: username
    passwordKey: password

  ingress:
    enabled: true
    ingressClassName: nginx-internal
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod-dns
      external-dns.alpha.kubernetes.io/enabled: "true"
    hosts:
      - &host grafana.internal.grigri.cloud
    tls:
      - secretName: grafana-general-tls
        hosts:
          - *host
  plugins:
    - grafana-piechart-panel
  persistence:
    enabled: false
  inMemory:
    enabled: true
    ## The maximum usage on memory medium EmptyDir would be
    ## the minimum value between the SizeLimit specified
    ## here and the sum of memory limits of all containers in a pod
    ##
    sizeLimit: 256Mi
  additionalDataSources:
    - name: Loki
      type: loki
      url: http://loki.loki:3100
      access: proxy
      isDefault: false
      version: 1

  sidecar:
    resources:
      limits:
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 64Mi
    dashboards:
      enabled: true
      label: grafana_dashboard
      resource: configmap
      folderAnnotation: grafana.grafana.com/dashboards.target-directory
      provider:
        foldersFromFilesStructure: true
      annotations:
        grafana.grafana.com/dashboards.target-directory: "/tmp/dashboards/kubernetes"
      searchNamespace: ALL

  envFromSecret: grafana-secret

  grafana.ini:
    server:
      root_url: https://grafana.internal.grigri.cloud

    log:
      level: info

    auth.generic_oauth:
      enabled: true
      allow_sign_up: true
      name: idm.grigri.cloud
      client_id: $__env{GRAFANA_SSO_CLIENT_ID}
      client_secret: $__env{GRAFANA_SSO_CLIENT_SECRET}
      scopes: openid profile email
      auth_url: https://idm.grigri.cloud/ui/oauth2
      token_url: https://idm.grigri.cloud/oauth2/token
      api_url: https://idm.grigri.cloud/oauth2/openid/grafana/userinfo
      use_pkce: true
      groups_attribute_path: scopes
      name_attribute_path: preferred_username
      role_attribute_path: contains(scopes[*], 'admin') && 'Admin' || 'Viewer'

  revisionHistoryLimit: 2

coreDns:
  enabled: true
  service:
    selector:
      k8s-app: kube-dns

kubeEtcd:
  enabled: false

kubeScheduler:
  enabled: false

kubeProxy:
  enabled: false

kubeControllerManager:
  enabled: false

prometheusOperator:
  # https://github.com/helm/charts/issues/19147
  admissionWebhooks:
    enabled: false
  tls:
    enabled: false

  resources:
    limits:
      memory: 200Mi
    requests:
      cpu: 100m
      memory: 100Mi

prometheus-node-exporter:
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: instance
## smart disk data metrics
#  extraArgs:
#    - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
#    - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
#    # this is the new line
#    - --collector.textfile.directory=/host/root/var/log/prometheus

prometheus:
  prometheusSpec:
    replicas: 1
    retentionSize: 30GB
    retention: 30d
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 4
        memory: 2.5Gi

    # remove need of label `release: monitoring`
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    scrapeConfigSelectorNilUsesHelmValues: false

    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: zfs-local-dataset
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 150Gi

  ingress:
    enabled: true
    ingressClassName: nginx-internal
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod-dns
      external-dns.alpha.kubernetes.io/enabled: "true"
    hosts:
      - prometheus.internal.grigri.cloud
    paths:
      - /
    tls:
      - secretName: prometheus-general-tls
        hosts:
          - prometheus.internal.grigri.cloud
